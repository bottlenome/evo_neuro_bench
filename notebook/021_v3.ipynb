{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a167d80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Minimal Evo-Neuro Benchmark (Colab-ready, single cell)\n",
    "# Models: CnidarianNerveNet / SegmentedGanglia / FishBrain / HumanExecutive\n",
    "# Tasks : HD-Jellyfish (SL, 3-way) + Reversal (RL, 2AFC)\n",
    "# ===============================================================\n",
    "\n",
    "import math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "def set_seed(seed=0):\n",
    "    import os, numpy as np, torch, random\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -----------------------------\n",
    "# Global sensory dimensions (fixed across models)\n",
    "# -----------------------------\n",
    "D_VISION, D_OLFACT, D_SOMATO, D_AUDIT, D_PROP = 128, 32, 60, 64, 16\n",
    "INPUT_DIM = D_VISION + D_OLFACT + D_SOMATO + D_AUDIT + D_PROP\n",
    "\n",
    "# -----------------------------\n",
    "# Obs helper (all modalities; allow overwrite)\n",
    "# -----------------------------\n",
    "def make_obs(batch=1, device=\"cpu\",\n",
    "             vision: torch.Tensor=None,\n",
    "             olfaction: torch.Tensor=None,\n",
    "             somato: torch.Tensor=None,\n",
    "             auditory: torch.Tensor=None,\n",
    "             proprio: torch.Tensor=None) -> Dict[str, torch.Tensor]:\n",
    "    obs = {\n",
    "        \"vision\": torch.randn(batch, D_VISION, device=device),\n",
    "        \"olfaction\": torch.randn(batch, D_OLFACT, device=device),\n",
    "        \"somatosensory\": torch.randn(batch, D_SOMATO, device=device),\n",
    "        \"auditory\": torch.randn(batch, D_AUDIT, device=device),\n",
    "        \"proprioception\": torch.randn(batch, D_PROP, device=device),\n",
    "    }\n",
    "    if vision  is not None: obs[\"vision\"] = vision\n",
    "    if olfaction is not None: obs[\"olfaction\"] = olfaction\n",
    "    if somato  is not None: obs[\"somatosensory\"] = somato\n",
    "    if auditory is not None: obs[\"auditory\"] = auditory\n",
    "    if proprio is not None:  obs[\"proprioception\"] = proprio\n",
    "    return obs\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny block\n",
    "# -----------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, out_dim, depth=2, act=nn.ReLU):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_dim, hidden), act()]\n",
    "        for _ in range(max(0, depth-2)):\n",
    "            layers += [nn.Linear(hidden, hidden), act()]\n",
    "        layers += [nn.Linear(hidden, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# ===============================================================\n",
    "# Models (each forward returns dict(motor=Tensor[B, motor_dim]))\n",
    "# ===============================================================\n",
    "\n",
    "# 1) Cnidarian: fixed random features (+small plastic head)\n",
    "class CnidarianNerveNet(nn.Module):\n",
    "    \"\"\"\n",
    "    生物の拡散神経網の素朴近似：入力→固定ランダム射影→tanh→小さな可塑ヘッド。\n",
    "    ユニバーサル近似器になりにくい容量に制限。\n",
    "    \"\"\"\n",
    "    def __init__(self, motor_dim=8, feat_dim=48):\n",
    "        super().__init__()\n",
    "        with torch.no_grad():\n",
    "            W = torch.randn(INPUT_DIM, feat_dim) / math.sqrt(INPUT_DIM)\n",
    "            b = torch.zeros(feat_dim)\n",
    "        self.register_buffer(\"W\", W)\n",
    "        self.register_buffer(\"b\", b)\n",
    "        self.head = nn.Linear(feat_dim, motor_dim)\n",
    "    def forward(self, x):\n",
    "        z = torch.cat([x[\"vision\"], x[\"olfaction\"], x[\"somatosensory\"], x[\"auditory\"], x[\"proprioception\"]], -1)\n",
    "        h = torch.tanh(z @ self.W + self.b)  # [B,feat]\n",
    "        return {\"motor\": torch.tanh(self.head(h))}\n",
    "\n",
    "# 2) Segmented ganglia: somatoを体節に分割し局所制御+全身座標\n",
    "class SegmentedGanglia(nn.Module):\n",
    "    def __init__(self, segments=6, motor_per_seg=2):\n",
    "        super().__init__()\n",
    "        assert D_SOMATO % segments == 0\n",
    "        self.segments = segments\n",
    "        self.local_dim = D_SOMATO // segments\n",
    "        self.coord = MLP(D_VISION + D_OLFACT + D_AUDIT + D_PROP, 64, 16, depth=2)\n",
    "        self.controllers = nn.ModuleList([\n",
    "            MLP(self.local_dim + 16, 64, motor_per_seg, depth=2) for _ in range(segments)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        B = x[\"somatosensory\"].size(0)\n",
    "        s = x[\"somatosensory\"].view(B, self.segments, self.local_dim)\n",
    "        c = self.coord(torch.cat([x[\"vision\"], x[\"olfaction\"], x[\"auditory\"], x[\"proprioception\"]], -1))\n",
    "        outs = [self.controllers[i](torch.cat([s[:,i,:], c], -1)) for i in range(self.segments)]\n",
    "        motor = torch.tanh(torch.cat(outs, -1))  # [B, segments*motor_per_seg]\n",
    "        return {\"motor\": motor}\n",
    "\n",
    "# helper modules\n",
    "class BasalGanglia(nn.Module):\n",
    "    def __init__(self, in_dim, motor_dim):\n",
    "        super().__init__()\n",
    "        self.pi = MLP(in_dim, 128, motor_dim, depth=2)\n",
    "        self.g  = MLP(in_dim, 64,  motor_dim, depth=2)\n",
    "    def forward(self, z):\n",
    "        return self.pi(z) * torch.softmax(self.g(z), -1)\n",
    "\n",
    "class Cerebellum(nn.Module):\n",
    "    def __init__(self, sensory_dim, motor_dim):\n",
    "        super().__init__()\n",
    "        self.fm = MLP(sensory_dim + motor_dim, 128, motor_dim, depth=2)\n",
    "    def forward(self, sensory, intended):\n",
    "        return 0.2*torch.tanh(self.fm(torch.cat([sensory, intended], -1)))\n",
    "\n",
    "# 2) Segmented ganglia (修正版): 各体節が somato の局所情報だけで制御\n",
    "class SegmentedGangliaRestricted(nn.Module):\n",
    "    \"\"\"\n",
    "    修正版:\n",
    "    - 各体節は自分の somatosensory 入力だけで motor を生成\n",
    "    - グローバル情報 (vision, olfact, auditory, proprio) は利用できない\n",
    "    - 本来の生物的 segmental ganglia に近く、Detour のような空間推論はできないはず\n",
    "    \"\"\"\n",
    "    def __init__(self, segments=6, motor_per_seg=2):\n",
    "        super().__init__()\n",
    "        assert D_SOMATO % segments == 0\n",
    "        self.segments = segments\n",
    "        self.local_dim = D_SOMATO // segments\n",
    "        self.controllers = nn.ModuleList([\n",
    "            MLP(self.local_dim, 32, motor_per_seg, depth=2)  # 小さめ MLP\n",
    "            for _ in range(segments)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x[\"somatosensory\"].size(0)\n",
    "        s = x[\"somatosensory\"].view(B, self.segments, self.local_dim)\n",
    "        outs = [self.controllers[i](s[:, i, :]) for i in range(self.segments)]\n",
    "        motor = torch.tanh(torch.cat(outs, -1))  # [B, segments*motor_per_seg]\n",
    "        return {\"motor\": motor}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ReflexArc: 局所反射弓（somatosensory → motor）\n",
    "# ---------------------------------------------\n",
    "class ReflexArc(nn.Module):\n",
    "    def __init__(self, sensory_dim, motor_dim, hidden=16):\n",
    "        super().__init__()\n",
    "        self.afferent = nn.Linear(sensory_dim, hidden)\n",
    "        self.interneuron = nn.ReLU()\n",
    "        self.efferent = nn.Linear(hidden, motor_dim)\n",
    "\n",
    "    def forward(self, sensory_input):\n",
    "        h = self.interneuron(self.afferent(sensory_input))\n",
    "        motor = torch.tanh(self.efferent(h))\n",
    "        return motor\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# BrachialGanglion: 腕ガングリオン\n",
    "# ReflexArc と中央脳入力を学習可能パラメータで統合\n",
    "# ---------------------------------------------\n",
    "class BrachialGanglion(nn.Module):\n",
    "    def __init__(self, sensory_dim, central_dim, motor_dim):\n",
    "        super().__init__()\n",
    "        self.reflex_arc = ReflexArc(sensory_dim, motor_dim)\n",
    "        self.reflex_weight = nn.Parameter(torch.tensor(1.0))    # reflexシナプス強度\n",
    "        self.central_fc = nn.Linear(central_dim, motor_dim)\n",
    "        self.central_weight = nn.Parameter(torch.tensor(1.0))   # centralシナプス強度\n",
    "\n",
    "    def forward(self, sensory_input, central_cmd):\n",
    "        reflex_out = self.reflex_arc(sensory_input)\n",
    "        central_out = torch.tanh(self.central_fc(central_cmd))\n",
    "        # 学習可能な重みで加算統合\n",
    "        motor = self.reflex_weight * reflex_out + self.central_weight * central_out\n",
    "        return motor\n",
    "\n",
    "\n",
    "# ---- 共通ユーティリティ ----\n",
    "class EIBlock(nn.Module):\n",
    "    \"\"\"皮質カラムの極小モデル: 興奮Eと抑制Iの相互作用（1ステップ離散化）\"\"\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.W_e = nn.Linear(d, d, bias=False)  # E←E\n",
    "        self.W_i = nn.Linear(d, d, bias=False)  # I←E\n",
    "        self.U_e = nn.Linear(d, d, bias=False)  # E←I（抑制）\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.7))  # Eの慣性\n",
    "        self.beta  = nn.Parameter(torch.tensor(0.5))  # Iの慣性\n",
    "        self.ln_e = nn.LayerNorm(d); self.ln_i = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, e, i, inp=None):\n",
    "        # inpは外部入力（視覚・体性感覚などの投射を想定）\n",
    "        if inp is None: inp = 0.0\n",
    "        e_new = self.ln_e(self.alpha*e + self.W_e(e) - self.U_e(i) + inp)\n",
    "        i_new = self.ln_i(self.beta *i + self.W_i(e))\n",
    "        return torch.relu(e_new), torch.relu(i_new)\n",
    "\n",
    "class ThalamicRelay(nn.Module):\n",
    "    \"\"\"視床リレー核: 皮質E活動を受け、選択的に再入力（可変ゲイン）\"\"\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.relay = nn.Linear(d, d, bias=False)\n",
    "        self.gain  = nn.Parameter(torch.tensor(0.8))\n",
    "\n",
    "    def forward(self, cortical_e):\n",
    "        return self.gain * self.relay(cortical_e)\n",
    "\n",
    "class BasalGangliaGate(nn.Module):\n",
    "    \"\"\"BGゲート: コンテキストにより視床出力を選択的に通す（抑制的出力の近似）\"\"\"\n",
    "    def __init__(self, d_ctx, d):\n",
    "        super().__init__()\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(d_ctx, d), nn.Tanh(),\n",
    "            nn.Linear(d, d), nn.Sigmoid()  # 0~1のゲート\n",
    "        )\n",
    "\n",
    "    def forward(self, ctx, thalamus_out):\n",
    "        g = self.policy(ctx)        # [B,d]\n",
    "        return g * thalamus_out, g  # 出力とゲインマップ\n",
    "\n",
    "\n",
    "class CephalopodBrainV3(nn.Module):\n",
    "    \"\"\"\n",
    "    頭足類モデル（再入ループ付き）\n",
    "    - optic lobe（視葉）: 視覚入力と垂直葉出力との再帰相互作用\n",
    "    - vertical lobe（垂直葉）: 視覚＋体性感覚の連合、抑制競合付きHebbian学習\n",
    "    - peduncle lobe（柄葉）: 小脳様前向きモデル（誤差補正）\n",
    "    - brachial ganglia（腕神経節）: 反射経路＋中央コマンド統合\n",
    "    \"\"\"\n",
    "    def __init__(self, arms=8, central_dim=64, motor_dim=4, re_loops=3):\n",
    "        super().__init__()\n",
    "        self.arms = arms\n",
    "        self.central_dim = central_dim\n",
    "        self.motor_dim = motor_dim\n",
    "        self.re_loops = re_loops\n",
    "        self.sensory_dim = D_SOMATO\n",
    "        self.visual_dim = D_VISION\n",
    "\n",
    "        # --- 視葉 ---\n",
    "        self.optic_in = nn.Linear(self.visual_dim, central_dim)\n",
    "        self.optic_rec = nn.Linear(central_dim, central_dim, bias=False)  # 再入ループ入力\n",
    "        self.optic_act = nn.Tanh()\n",
    "\n",
    "        # --- 垂直葉 ---\n",
    "        self.vert_in = nn.Linear(central_dim + self.sensory_dim, central_dim)\n",
    "        self.vert_self = nn.Linear(central_dim, central_dim, bias=False)   # 再帰\n",
    "        self.vert_inhib = nn.Linear(central_dim, central_dim, bias=False)  # 抑制項\n",
    "        self.beta = nn.Parameter(torch.tensor(0.3))   # Hebbian強度\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.5))  # 抑制強度\n",
    "        self.vert_norm = nn.LayerNorm(central_dim)\n",
    "\n",
    "        # --- 柄葉（小脳様） ---\n",
    "        self.peduncle_lobe = nn.Sequential(\n",
    "            nn.Linear(central_dim, central_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # --- 腕ガングリオン ---\n",
    "        self.brachial_ganglia = nn.ModuleList([\n",
    "            BrachialGanglion(self.sensory_dim, central_dim, motor_dim)\n",
    "            for _ in range(self.arms)\n",
    "        ])\n",
    "\n",
    "        self.out_act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: {\"vision\": [B, Dv], \"somatosensory\": [B, Ds]}\n",
    "        return: {\"motor\": [B, arms*motor_dim], \"central_command\": [B, Dc]}\n",
    "        \"\"\"\n",
    "        B = x[\"vision\"].size(0)\n",
    "\n",
    "        # 初期視葉出力\n",
    "        optic = torch.tanh(self.optic_in(x[\"vision\"]))\n",
    "        vert = torch.zeros_like(optic)\n",
    "\n",
    "        # --- 再入ループ (optic <-> vertical) ---\n",
    "        for _ in range(self.re_loops):\n",
    "            # 垂直葉更新：視覚＋体性感覚＋再帰\n",
    "            vert_in = torch.cat([optic, x[\"somatosensory\"]], dim=-1)\n",
    "            h = torch.tanh(self.vert_in(vert_in))\n",
    "            inhib = self.vert_inhib(vert)   # 抑制信号\n",
    "            vert = torch.tanh(self.vert_self(vert) + h - self.gamma * inhib)\n",
    "            # Hebbian項（自己相関強調）\n",
    "            vert = vert + self.beta * (vert * h)\n",
    "            vert = self.vert_norm(vert)\n",
    "\n",
    "            # 視葉更新（再入）\n",
    "            optic = self.optic_act(self.optic_in(x[\"vision\"]) + self.optic_rec(vert))\n",
    "\n",
    "        # 柄葉（小脳様）\n",
    "        central_cmd = self.peduncle_lobe(vert)\n",
    "\n",
    "        # 腕反射経路（局所統合）\n",
    "        motors = []\n",
    "        for i in range(self.arms):\n",
    "            mi = self.brachial_ganglia[i](x[\"somatosensory\"], central_cmd)\n",
    "            motors.append(mi)\n",
    "        motor = self.out_act(torch.cat(motors, dim=-1))\n",
    "        return {\"motor\": motor, \"central_command\": central_cmd}\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Fish (統一フロー版): optic tectum → ventral telencephalon (BG-like)\n",
    "#                      → cerebellum → spinal cord (integration) → motor\n",
    "#   ＋ 反射系: somatosensory/proprioception → spinal interneurons → motor\n",
    "#   両経路は脊髄で学習的に調停（ゲーティング）される\n",
    "# ===============================================================\n",
    "class SpinalMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    脊髄での反射/随意の調停を抽象化:\n",
    "      gate ∈ [0,1]^{motor_dim} を学習し、motor = tanh( gate*reflex + (1-gate)*volitional )\n",
    "    gate は（文脈依存）= g(somato, proprio, reflex, volitional)\n",
    "    \"\"\"\n",
    "    def __init__(self, motor_dim: int, ctx_dim: int):\n",
    "        super().__init__()\n",
    "        # gateを出す小さなMLP（文脈: somato+proprio と両ドライブを適度に見る）\n",
    "        hidden = 128\n",
    "        self.ctx_proj = MLP(ctx_dim, hidden, hidden, depth=2)\n",
    "        self.g_head   = nn.Linear(hidden, motor_dim)\n",
    "        # 初期は随意寄り(=gate小さめ)にしたい場合はbiasを負に初期化しても良い\n",
    "        nn.init.constant_(self.g_head.bias, 0.0)\n",
    "\n",
    "    def forward(self, reflex_drive, volitional_drive, ctx):\n",
    "        h = self.ctx_proj(ctx)\n",
    "        gate = torch.sigmoid(self.g_head(h))           # [B, motor_dim] in [0,1]\n",
    "        motor = torch.tanh(gate * reflex_drive + (1.0 - gate) * volitional_drive)\n",
    "        return motor, gate\n",
    "\n",
    "\n",
    "class FishBrainV3(nn.Module):\n",
    "    \"\"\"\n",
    "    解剖学マッピング:\n",
    "      - optic_tectum (視蓋): 視覚統合 + ventral_telencephalonからの再入\n",
    "      - ventral_telencephalon (前脳腹側部): 行動選択・ゲーティング\n",
    "      - cerebellum (小脳様): 誤差補正・協調\n",
    "      - spinal_interneurons: 反射ドライブ\n",
    "      - spinal_mixer: 反射と随意の学習的調停\n",
    "    改修点:\n",
    "      - optic_tectum と ventral_telencephalon 間に再入ループを導入\n",
    "      - ventral_telencephalon 出力を視蓋入力へ再投射（期待・文脈バイアス）\n",
    "      - 反射経路は従来通り維持\n",
    "    \"\"\"\n",
    "    def __init__(self, motor_dim=12, loops=2, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.loops = loops\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # --- 視蓋 (optic tectum) ---\n",
    "        self.optic_in = nn.Linear(D_VISION, hidden_dim)\n",
    "        self.optic_rec = nn.Linear(motor_dim, hidden_dim, bias=False)  # 再入投射 (BG出力)\n",
    "        self.optic_act = nn.Tanh()\n",
    "\n",
    "        # --- 前脳腹側部 (ventral telencephalon; BG様) ---\n",
    "        self.bg_in = nn.Linear(hidden_dim + D_OLFACT + D_PROP, motor_dim)\n",
    "        self.bg_gate = nn.Linear(motor_dim, motor_dim, bias=False)\n",
    "        self.bg_act = nn.Tanh()\n",
    "\n",
    "        # --- 小脳様補正 ---\n",
    "        self.cerebellum = Cerebellum(D_SOMATO + D_PROP, motor_dim)\n",
    "\n",
    "        # --- 反射経路 ---\n",
    "        self.spinal_interneurons = MLP(D_SOMATO + D_PROP, 128, motor_dim, depth=2)\n",
    "\n",
    "        # --- 脊髄統合 ---\n",
    "        ctx_dim = D_SOMATO + D_PROP\n",
    "        self.spinal_mixer = SpinalMixer(motor_dim=motor_dim, ctx_dim=ctx_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x[\"vision\"].size(0)\n",
    "        # 初期視蓋表象\n",
    "        optic = self.optic_act(self.optic_in(x[\"vision\"]))\n",
    "        bg_out = torch.zeros(B, self.hidden_dim, device=optic.device)\n",
    "\n",
    "        # --- optic–BG再入ループ ---\n",
    "        for _ in range(self.loops):\n",
    "            bg_in = torch.cat([optic, x[\"olfaction\"], x[\"proprioception\"]], dim=-1)\n",
    "            bg_drive = self.bg_act(self.bg_in(bg_in))\n",
    "            optic = self.optic_act(self.optic_in(x[\"vision\"]) + self.optic_rec(bg_drive))\n",
    "\n",
    "        # --- 小脳経路 ---\n",
    "        z_cb = torch.cat([x[\"somatosensory\"], x[\"proprioception\"]], -1)\n",
    "        volitional = bg_drive + self.cerebellum(z_cb, bg_drive)\n",
    "\n",
    "        # --- 反射経路 ---\n",
    "        reflex = self.spinal_interneurons(torch.cat([x[\"somatosensory\"], x[\"proprioception\"]], -1))\n",
    "\n",
    "        # --- 脊髄統合 ---\n",
    "        ctx = torch.cat([x[\"somatosensory\"], x[\"proprioception\"]], -1)\n",
    "        motor, gate = self.spinal_mixer(reflex, volitional, ctx)\n",
    "\n",
    "        return {\"motor\": motor, \"gate\": gate}\n",
    "\n",
    "\n",
    "class HumanCortexV4(nn.Module):\n",
    "    \"\"\"\n",
    "    解剖学マッピング:\n",
    "      - 感覚統合皮質 (V/S/P/A)\n",
    "      - PFC: 再帰的作業表象 + Thalamic再入 + BGゲート\n",
    "      - 海馬様統合: 文脈保持\n",
    "      - 小脳: 予測誤差補正\n",
    "      - 脊髄反射＋統合: Reflex/Volitionalの調停\n",
    "    改修:\n",
    "      - PFC ↔ Thalamus の再入ループを導入\n",
    "      - Thalamus出力はBGゲートを介してPFCへ再投射\n",
    "      - Descending制御ゲインをSpinalMixerに追加\n",
    "    \"\"\"\n",
    "    def __init__(self, motor_dim=20, d_emb=64, wm_dim=128, loops=2):\n",
    "        super().__init__()\n",
    "        self.motor_dim = motor_dim\n",
    "        self.wm_dim = wm_dim\n",
    "        self.d_emb = d_emb\n",
    "        self.loops = loops\n",
    "\n",
    "        # 感覚埋め込み\n",
    "        self.tv = MLP(D_VISION, 128, d_emb, depth=2)\n",
    "        self.ts = MLP(D_SOMATO, 128, d_emb, depth=2)\n",
    "        self.tp = MLP(D_PROP,   64,  d_emb, depth=2)\n",
    "        self.ta = MLP(D_AUDIT,  64,  d_emb, depth=2)\n",
    "\n",
    "        # PFC (再入対象)\n",
    "        self.pfc_in = nn.Linear(d_emb * 4, wm_dim)\n",
    "        self.pfc_rec = nn.Linear(wm_dim, wm_dim, bias=False)\n",
    "        self.pfc_norm = nn.LayerNorm(wm_dim)\n",
    "\n",
    "        # Thalamus + BG Gate\n",
    "        self.thalamus = nn.Linear(wm_dim, wm_dim, bias=False)\n",
    "        self.bg_gate = nn.Sequential(\n",
    "            nn.Linear(D_SOMATO + D_PROP, wm_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(wm_dim, wm_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # 海馬様統合（既存）\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=wm_dim, nhead=4, dim_feedforward=256)\n",
    "        self.hippocampal = nn.TransformerEncoder(enc_layer, num_layers=1)\n",
    "        self.h_norm = nn.LayerNorm(wm_dim)\n",
    "\n",
    "        # 小脳様補正\n",
    "        self.cerebellum = Cerebellum(D_SOMATO + D_PROP, motor_dim)\n",
    "\n",
    "        # 反射系\n",
    "        self.spinal_reflex = MLP(D_SOMATO + D_PROP, 128, motor_dim, depth=2)\n",
    "        self.spinal_mixer = SpinalMixer(motor_dim, D_SOMATO + D_PROP)\n",
    "\n",
    "        # 下行性ゲイン\n",
    "        self.desc_gain = nn.Sequential(nn.Linear(wm_dim, motor_dim), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x[\"vision\"].size(0)\n",
    "\n",
    "        # 感覚統合\n",
    "        s = torch.cat([\n",
    "            self.tv(x[\"vision\"]),\n",
    "            self.ts(x[\"somatosensory\"]),\n",
    "            self.tp(x[\"proprioception\"]),\n",
    "            self.ta(x[\"auditory\"])\n",
    "        ], dim=-1)\n",
    "\n",
    "        # PFC初期化\n",
    "        pfc = torch.tanh(self.pfc_in(s))\n",
    "        ctx_bg = torch.cat([x[\"somatosensory\"], x[\"proprioception\"]], -1)\n",
    "\n",
    "        # --- 再入ループ (PFC↔Thalamus with BG gate) ---\n",
    "        for _ in range(self.loops):\n",
    "            th = self.thalamus(pfc)\n",
    "            g = self.bg_gate(ctx_bg)\n",
    "            pfc = torch.tanh(self.pfc_in(s) + self.pfc_rec(th * g))\n",
    "            pfc = self.pfc_norm(pfc)\n",
    "\n",
    "        # 海馬様統合\n",
    "        mem = self.hippocampal(pfc.unsqueeze(0)).squeeze(0)\n",
    "        mem = self.h_norm(mem)\n",
    "\n",
    "        # 小脳予測補正\n",
    "        cb_inp = torch.cat([x[\"somatosensory\"], x[\"proprioception\"]], -1)\n",
    "        volitional = self.cerebellum(cb_inp, self.desc_gain(mem))\n",
    "\n",
    "        # 反射ルート\n",
    "        reflex = self.spinal_reflex(torch.cat([x[\"somatosensory\"], x[\"proprioception\"]], -1))\n",
    "\n",
    "        # 脊髄統合\n",
    "        ctx = torch.cat([x[\"somatosensory\"], x[\"proprioception\"]], -1)\n",
    "        motor, gate = self.spinal_mixer(reflex, volitional, ctx)\n",
    "\n",
    "        return {\"motor\": motor, \"gate\": gate, \"wm\": mem}\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Task: Peristalsis (蠕動波出力) — 回帰（MSE）\n",
    "# 目標：各体節のモータ出力が位相差φのある正弦波を形成\n",
    "# 文献背景：体節CPG/体節間結合（例：Friesen & Pearce 2007, leech locomotor circuits）\n",
    "# ===============================================================\n",
    "class PeristalsisDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    回帰タスク：motor_target ∈ R^(motor_dim)\n",
    "    入力obsは somato と proprio に時間・目標速度などを符号化（単純ノイズでもOK）。\n",
    "    \"\"\"\n",
    "    def __init__(self, motor_dim, n_samples=6000, A=1.0, omega=0.4, phi=0.8, noise=0.05, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.motor_dim = motor_dim\n",
    "        self.A, self.omega, self.phi = A, omega, phi\n",
    "        self.noise = noise\n",
    "        self.T = []\n",
    "        self.targets = []\n",
    "        Xv, Xo, Xa, Xp, Xs = [], [], [], [], []\n",
    "        for _ in range(n_samples):\n",
    "            t = random.uniform(0, 2*math.pi)\n",
    "            target = torch.tensor(\n",
    "                [A*math.sin(omega*t + phi*i) for i in range(motor_dim)],\n",
    "                device=device, dtype=torch.float32\n",
    "            )\n",
    "            # 観測は最小限：somato/proprioに t, omega, phi を雑に埋める（モデル間で公平）\n",
    "            v = torch.randn(D_VISION, device=device) * self.noise\n",
    "            o = torch.randn(D_OLFACT, device=device) * self.noise\n",
    "            a = torch.randn(D_AUDIT,  device=device) * self.noise\n",
    "            p = torch.randn(D_PROP,   device=device) * self.noise\n",
    "            s = torch.randn(D_SOMATO, device=device) * self.noise\n",
    "            # t, omega, phi を少数の次元に書き込む\n",
    "            p[:3] = torch.tensor([t, omega, phi], device=device)\n",
    "            Xv.append(v); Xo.append(o); Xa.append(a); Xp.append(p); Xs.append(s)\n",
    "            self.targets.append(target)\n",
    "        self.V = torch.stack(Xv); self.O = torch.stack(Xo)\n",
    "        self.Au = torch.stack(Xa); self.P = torch.stack(Xp); self.S = torch.stack(Xs)\n",
    "        self.Y = torch.stack(self.targets)\n",
    "\n",
    "    def __len__(self): return self.V.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        obs = {\n",
    "            \"vision\": self.V[idx],\n",
    "            \"olfaction\": self.O[idx],\n",
    "            \"auditory\": self.Au[idx],\n",
    "            \"proprioception\": self.P[idx],\n",
    "            \"somatosensory\": self.S[idx],\n",
    "        }\n",
    "        return obs, self.Y[idx]\n",
    "\n",
    "@torch.no_grad()\n",
    "def phase_corr(y_pred, y_true):\n",
    "    # 各体節系列の位相整合をざっくり測る（相関係数の平均）\n",
    "    num = (y_pred * y_true).sum(-1)\n",
    "    den = (y_pred.pow(2).sum(-1).sqrt() * y_true.pow(2).sum(-1).sqrt() + 1e-9)\n",
    "    return (num/den).mean().item()\n",
    "\n",
    "def train_peristalsis(base_model: nn.Module, device=\"cpu\", epochs=5, batch_size=128, lr=1e-3):\n",
    "    dummy = make_obs(batch=2, device=device)\n",
    "    motor_dim = base_model(dummy)[\"motor\"].size(-1)\n",
    "    ds_tr = PeristalsisDataset(motor_dim, n_samples=6000, device=device)\n",
    "    ds_ev = PeristalsisDataset(motor_dim, n_samples=800, device=device)\n",
    "    dl = torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    opt = torch.optim.Adam(base_model.parameters(), lr=lr)\n",
    "    curve = []  # (trial_idx, \"accuracy\") の形式に近づける\n",
    "    trial_counter = 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        for obs, target in dl:\n",
    "            trial_counter += len(target)\n",
    "            for k in obs: obs[k] = obs[k].to(device)\n",
    "            target = target.to(device)\n",
    "            pred = base_model(obs)[\"motor\"]\n",
    "            loss = F.mse_loss(pred, target)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            # phaseCorr を「精度指標」とみなして curve に記録\n",
    "            acc_batch = phase_corr(pred, target)  # 0〜1 に近い値\n",
    "            curve.append((trial_counter, acc_batch))\n",
    "        print(f\"[Peristalsis] epoch={ep} loss={loss.item():.4f}\")\n",
    "\n",
    "    # 評価\n",
    "    base_model.eval()\n",
    "    mse_tot, corr_tot, N = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for obs, target in ds_ev:\n",
    "            for k in obs: obs[k] = obs[k].unsqueeze(0).to(device)\n",
    "            target = target.unsqueeze(0).to(device)\n",
    "            pred = base_model(obs)[\"motor\"]\n",
    "            mse_tot += F.mse_loss(pred, target).item()\n",
    "            corr_tot += phase_corr(pred.squeeze(0), target.squeeze(0))\n",
    "            N += 1\n",
    "    mse_eval = mse_tot/max(1,N)\n",
    "    corr_eval = corr_tot/max(1,N)\n",
    "    print(f\"[Peristalsis] eval MSE={mse_eval:.4f}, phaseCorr={corr_eval:.3f}\")\n",
    "\n",
    "    return mse_eval, corr_eval, curve\n",
    "\n",
    "# ===============================================================\n",
    "# Task: Local Reflex（体節接触反射）— 3クラス分類\n",
    "# 文献背景：Bässler (1986) 他、stick insect などの局所反射制御\n",
    "# ラベル: 0=左回避, 1=直進維持, 2=右回避（簡略化）\n",
    "# ===============================================================\n",
    "class LocalReflexDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, n_samples=6000, segments=6, contact_p=0.4, noise=0.05, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.segments = segments\n",
    "        assert D_SOMATO % segments == 0\n",
    "        self.local_dim = D_SOMATO // segments\n",
    "        Xv, Xo, Xa, Xp, Xs, Y = [], [], [], [], [], []\n",
    "        for _ in range(n_samples):\n",
    "            # どの体節に接触が入るか（複数もありうる）\n",
    "            contacts = [1 if random.random()<contact_p else 0 for _ in range(segments)]\n",
    "            # 左群（前半の体節）に接触が偏れば左回避、右群（後半）なら右回避、どちらもなければ直進\n",
    "            left_sum  = sum(contacts[:segments//2])\n",
    "            right_sum = sum(contacts[segments//2:])\n",
    "            if left_sum>right_sum and left_sum>0: y = 2  # 左接触多 → 右回避\n",
    "            elif right_sum>left_sum and right_sum>0: y = 0 # 右接触多 → 左回避\n",
    "            else: y = 1  # 直進\n",
    "\n",
    "            # somato 符号化：各体節ブロックの最初の1次元に接触強度、それ以外はノイズ\n",
    "            s = torch.randn(D_SOMATO, device=device)*noise\n",
    "            for i,c in enumerate(contacts):\n",
    "                if c:\n",
    "                    start = i*self.local_dim\n",
    "                    s[start] = 1.0  # 接触フラグ\n",
    "\n",
    "            obs = {\n",
    "                \"vision\": torch.randn(D_VISION, device=device)*noise,   # 使わない\n",
    "                \"olfaction\": torch.randn(D_OLFACT, device=device)*noise,\n",
    "                \"auditory\": torch.randn(D_AUDIT, device=device)*noise,\n",
    "                \"proprioception\": torch.randn(D_PROP, device=device)*noise,\n",
    "                \"somatosensory\": s,\n",
    "            }\n",
    "            Xv.append(obs[\"vision\"]); Xo.append(obs[\"olfaction\"])\n",
    "            Xa.append(obs[\"auditory\"]); Xp.append(obs[\"proprioception\"]); Xs.append(s)\n",
    "            Y.append(y)\n",
    "        self.V = torch.stack(Xv); self.O = torch.stack(Xo); self.Au = torch.stack(Xa)\n",
    "        self.P = torch.stack(Xp); self.S = torch.stack(Xs)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self): return self.V.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        obs = {\n",
    "            \"vision\": self.V[idx],\n",
    "            \"olfaction\": self.O[idx],\n",
    "            \"auditory\": self.Au[idx],\n",
    "            \"proprioception\": self.P[idx],\n",
    "            \"somatosensory\": self.S[idx],\n",
    "        }\n",
    "        return obs, self.Y[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Adapter: auto head to n_actions; optional tiny memory (off by default)\n",
    "# ===============================================================\n",
    "class ModelAdapter(nn.Module):\n",
    "    def __init__(self, base: nn.Module, n_actions: int, use_memory: bool=False, mem_dim: int=64):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.n_actions = n_actions\n",
    "        self.use_memory = use_memory\n",
    "        self.mem_dim = mem_dim\n",
    "        self._head = None\n",
    "        self._mem = None\n",
    "        self._h = None\n",
    "    def _lazy_init(self, motor_dim, device):\n",
    "        if self.use_memory:\n",
    "            self._mem = nn.GRUCell(motor_dim, self.mem_dim).to(device)\n",
    "            self._head = nn.Linear(self.mem_dim, self.n_actions).to(device)\n",
    "        else:\n",
    "            self._head = nn.Linear(motor_dim, self.n_actions).to(device)\n",
    "    def reset_memory(self, B=1, device=\"cpu\"):\n",
    "        if self.use_memory and self._mem is not None:\n",
    "            self._h = torch.zeros(B, self.mem_dim, device=device)\n",
    "    def forward(self, obs: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        out = self.base(obs)\n",
    "        motor = out[\"motor\"]\n",
    "        if self._head is None:\n",
    "            self._lazy_init(motor.size(-1), motor.device)\n",
    "        if self.use_memory:\n",
    "            if (self._h is None) or (self._h.size(0) != motor.size(0)):\n",
    "                self.reset_memory(B=motor.size(0), device=motor.device)\n",
    "            self._h = self._mem(motor, self._h)\n",
    "            logits = self._head(self._h)\n",
    "        else:\n",
    "            logits = self._head(motor)\n",
    "        return logits, out\n",
    "\n",
    "# ===============================================================\n",
    "# Task A: HD-Jellyfish (high-dim cue; supervised 3-way)\n",
    "# ===============================================================\n",
    "V_BASE, S_BASE = 8, 16  # biologically plausible low-dim bases\n",
    "\n",
    "class FrozenObsMixer(nn.Module):\n",
    "    \"\"\"Low-dim bases -> fixed linear lift to Vision128 / Somato60.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with torch.no_grad():\n",
    "            Wv = torch.randn(V_BASE, D_VISION) / math.sqrt(V_BASE)\n",
    "            Ws = torch.randn(S_BASE, D_SOMATO) / math.sqrt(S_BASE)\n",
    "        self.register_buffer(\"Wv\", Wv)\n",
    "        self.register_buffer(\"Ws\", Ws)\n",
    "    @torch.no_grad()\n",
    "    def forward(self, v_base, s_base):\n",
    "        return v_base @ self.Wv, s_base @ self.Ws\n",
    "\n",
    "class HDJellyfishDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    label: 0=左へ回避, 1=直進, 2=右へ回避\n",
    "    - create low-dim cues on V_BASE and S_BASE, then lift to full dims.\n",
    "    - other modalities are noise (same for all models).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=8000, p_none=0.3, amp=(0.6,1.3), noise=0.1, device=\"cpu\", mixer=None):\n",
    "        self.device = device\n",
    "        self.noise = noise\n",
    "        self.mixer = mixer or FrozenObsMixer()\n",
    "        Xv, Xs, Xo, Xa, Xp, Y = [], [], [], [], [], []\n",
    "        for _ in range(n_samples):\n",
    "            r = random.random()\n",
    "            left, right = 0.0, 0.0\n",
    "            if r < p_none:\n",
    "                y = 1\n",
    "            else:\n",
    "                if random.random() < 0.5:\n",
    "                    left = random.uniform(*amp);  y = 2  # 右回避\n",
    "                else:\n",
    "                    right= random.uniform(*amp);  y = 0  # 左回避\n",
    "            v = torch.randn(V_BASE, device=device) * (noise*0.3)\n",
    "            half = V_BASE//2\n",
    "            if left>0:  v[:half]  += left\n",
    "            if right>0: v[half:]  += right\n",
    "            s = torch.randn(S_BASE, device=device)*(noise*0.3)\n",
    "            halfs=S_BASE//2\n",
    "            if left>0:  s[:halfs]  += 0.3*left\n",
    "            if right>0: s[halfs:]  += 0.3*right\n",
    "            V_full, S_full = self.mixer(v.unsqueeze(0), s.unsqueeze(0))\n",
    "            V_full = V_full.squeeze(0) + noise*torch.randn(D_VISION, device=device)\n",
    "            S_full = S_full.squeeze(0) + noise*torch.randn(D_SOMATO, device=device)\n",
    "            O = noise*torch.randn(D_OLFACT, device=device)\n",
    "            A = noise*torch.randn(D_AUDIT,  device=device)\n",
    "            P = noise*torch.randn(D_PROP,   device=device)\n",
    "            Xv.append(V_full); Xs.append(S_full); Xo.append(O); Xa.append(A); Xp.append(P); Y.append(y)\n",
    "        self.V = torch.stack(Xv); self.S = torch.stack(Xs)\n",
    "        self.O = torch.stack(Xo); self.A = torch.stack(Xa); self.P = torch.stack(Xp)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.long, device=device)\n",
    "    def __len__(self): return self.V.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        obs = {\n",
    "            \"vision\": self.V[idx],\n",
    "            \"olfaction\": self.O[idx],\n",
    "            \"somatosensory\": self.S[idx],\n",
    "            \"auditory\": self.A[idx],\n",
    "            \"proprioception\": self.P[idx],\n",
    "        }\n",
    "        return obs, self.Y[idx]\n",
    "\n",
    "# ===============================================================\n",
    "# Task B: Reversal Learning (2AFC RL)\n",
    "# ===============================================================\n",
    "class ReversalEnv:\n",
    "    def __init__(self, dim=D_VISION, noise=0.8, rev_at=1500, device=\"cpu\"):\n",
    "        self.dim, self.noise, self.rev_at, self.device = dim, noise, rev_at, device\n",
    "        self.mu = torch.stack([torch.randn(dim), torch.randn(dim)], 0).to(device)\n",
    "        self.t = 0\n",
    "    def step(self):\n",
    "        cls = random.randint(0,1)\n",
    "        v = self.mu[cls] + self.noise*torch.randn(self.dim, device=self.device)\n",
    "        obs = make_obs(vision=v.unsqueeze(0), device=self.device)\n",
    "        correct = cls if self.t < self.rev_at else 1 - cls\n",
    "        self.t += 1\n",
    "        return obs, correct\n",
    "\n",
    "# ===============================================================\n",
    "# Task: Detour (魚類空間認知タスクの再現)\n",
    "# ===============================================================\n",
    "# ===============================================================\n",
    "# Hard Detour 課題（Cnidarianでは解けない設計）\n",
    "# ===============================================================\n",
    "class HardDetourDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ゴールと障害物を2次元座標に配置し、相対関係から行動を決める課題。\n",
    "    label: 0=左, 1=直進, 2=右\n",
    "    - Cnidarian のような線形モデルでは非線形規則を表現できず失敗するはず。\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=5000, field_size=5, noise=0.1, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.noise = noise\n",
    "        self.samples = []\n",
    "        for _ in range(n_samples):\n",
    "            # ゴール座標（前方に配置）\n",
    "            gx, gy = random.randint(-field_size, field_size), field_size\n",
    "            # 障害物座標（ゴール手前にランダム配置）\n",
    "            ox, oy = random.randint(-field_size, field_size), random.randint(1, field_size)\n",
    "\n",
    "            # Visionベクトル初期化\n",
    "            v = torch.zeros(D_VISION, device=device)\n",
    "\n",
    "            # ゴールの位置を符号化（インデックスをmodで割り当て）\n",
    "            v[(gx + field_size) % D_VISION] = 1.0\n",
    "            # 障害物の位置を符号化（ゴールと異なるインデックスに負の信号）\n",
    "            v[(ox + oy + field_size) % D_VISION] = -1.0\n",
    "\n",
    "            # 正解ラベルの決定（非線形ルール）\n",
    "            if abs(ox - gx) < 2 and oy < gy:\n",
    "                # ゴール手前に障害物あり → 回り込みが必要\n",
    "                if ox <= 0:\n",
    "                    y = 2  # 障害物が左寄り → 右回り\n",
    "                else:\n",
    "                    y = 0  # 障害物が右寄り → 左回り\n",
    "            else:\n",
    "                # 障害物が進路にかかっていない → ゴール方向へ直進\n",
    "                if gx < -1: y = 0\n",
    "                elif gx > 1: y = 2\n",
    "                else: y = 1\n",
    "\n",
    "            # ノイズを加える\n",
    "            v += noise * torch.randn_like(v)\n",
    "\n",
    "            obs = {\n",
    "                \"vision\": v,\n",
    "                \"olfaction\": torch.randn(D_OLFACT, device=device) * noise,\n",
    "                \"auditory\": torch.randn(D_AUDIT, device=device) * noise,\n",
    "                \"proprioception\": torch.randn(D_PROP, device=device) * noise,\n",
    "                \"somatosensory\": torch.randn(D_SOMATO, device=device) * noise,\n",
    "            }\n",
    "            self.samples.append((obs, y))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "# ===============================================================\n",
    "# Jellyfish (HD-Jellyfish 回避タスク)\n",
    "# ===============================================================\n",
    "from collections import deque\n",
    "\n",
    "def train_hd_jellyfish(adapter: ModelAdapter, device=\"cpu\", epochs=3, batch_size=128,\n",
    "                       p_none=0.3, noise=0.1):\n",
    "    mixer = FrozenObsMixer().to(device)\n",
    "    ds_tr = HDJellyfishDataset(8000, p_none=p_none, noise=noise, device=device, mixer=mixer)\n",
    "    ds_ev = HDJellyfishDataset(1000, p_none=p_none, noise=noise, device=device, mixer=mixer)\n",
    "    dl = torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    opt = torch.optim.Adam(adapter.parameters(), lr=1e-3)\n",
    "    adapter.train()\n",
    "\n",
    "    curve = []  # (step, moving_avg_loss)\n",
    "    step = 0\n",
    "    window = 100\n",
    "    loss_window = deque(maxlen=window)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        total, cnt = 0.0, 0\n",
    "        for obs, y in dl:\n",
    "            step += 1\n",
    "            for k in obs: obs[k] = obs[k].to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits, _ = adapter(obs)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            total += loss.item()*y.size(0); cnt += y.size(0)\n",
    "\n",
    "            # 移動平均でカーブを記録\n",
    "            loss_window.append(loss.item())\n",
    "            avg_loss = sum(loss_window)/len(loss_window)\n",
    "            curve.append((step, avg_loss))\n",
    "\n",
    "        print(f\"[HD-Jellyfish] epoch={ep} loss={total/max(1,cnt):.3f}\")\n",
    "\n",
    "    # 評価\n",
    "    adapter.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(ds_ev)):\n",
    "            obs, y = ds_ev[i]\n",
    "            for k in obs: obs[k] = obs[k].unsqueeze(0).to(device)\n",
    "            y = int(y.item())\n",
    "            logits, _ = adapter(obs)\n",
    "            pred = logits.argmax(-1).item()\n",
    "            correct += int(pred == y); total += 1\n",
    "    acc = correct/max(1,total)\n",
    "    print(f\"[HD-Jellyfish] eval accuracy = {acc:.3f}\")\n",
    "\n",
    "    return acc, curve\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Reversal Learning (逐次強化学習タスク)\n",
    "# ===============================================================\n",
    "def train_reversal(adapter: ModelAdapter, steps=2000, lr=1e-3, device=\"cpu\"):\n",
    "    env = ReversalEnv(device=device)\n",
    "    opt = torch.optim.Adam(adapter.parameters(), lr=lr)\n",
    "    baseline = 0.0; beta=0.02\n",
    "\n",
    "    pre_acc, post_acc = [], []\n",
    "    curve = []  # (step, moving_avg_acc)\n",
    "\n",
    "    adapter.train(); adapter.reset_memory(1, device)\n",
    "\n",
    "    window = 100  # 移動平均用\n",
    "    acc_window = deque(maxlen=window)\n",
    "\n",
    "    for i in range(1, steps+1):\n",
    "        obs, correct = env.step()\n",
    "        logits, _ = adapter(obs)\n",
    "        dist = Categorical(logits=logits)  # 温度調整するなら logits/τ\n",
    "        act = dist.sample()\n",
    "        rew = 1.0 if int(act) == correct else 0.0\n",
    "        adv = rew - baseline\n",
    "\n",
    "        loss = -(dist.log_prob(act) * adv)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        baseline = (1 - beta) * baseline + beta * rew\n",
    "\n",
    "        # 精度記録\n",
    "        acc = 1.0 if int(act) == correct else 0.0\n",
    "        (pre_acc if i < env.rev_at else post_acc).append(acc)\n",
    "        acc_window.append(acc)\n",
    "        avg_acc = sum(acc_window) / len(acc_window)\n",
    "        curve.append((i, avg_acc))\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f\"[Reversal] step={i:4d} \"\n",
    "                  f\"pre_acc={sum(pre_acc[-500:])/max(1,len(pre_acc[-500:])):.2f} \"\n",
    "                  f\"post_acc={sum(post_acc[-500:])/max(1,len(post_acc[-500:])):.2f}\")\n",
    "\n",
    "    pa = sum(pre_acc[-500:]) / max(1, len(pre_acc[-500:]))\n",
    "    po = sum(post_acc[-500:]) / max(1, len(post_acc[-500:]))\n",
    "\n",
    "    return float(pa), float(po), curve\n",
    "\n",
    "\n",
    "def train_detour(adapter: ModelAdapter, device=\"cpu\", epochs=5, batch_size=128):\n",
    "    ds_tr = HardDetourDataset(4000, device=device)   # ★ HardDetour に変更\n",
    "    ds_ev = HardDetourDataset(1000, device=device)\n",
    "    dl = torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    opt = torch.optim.Adam(adapter.parameters(), lr=1e-3)\n",
    "    adapter.train()\n",
    "\n",
    "    curve = []\n",
    "    trial_counter = 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        total, cnt = 0.0, 0\n",
    "        for obs, y in dl:\n",
    "            trial_counter += len(y)\n",
    "            for k in obs:\n",
    "                obs[k] = obs[k].to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits, _ = adapter(obs)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(-1)\n",
    "                acc_batch = (pred == y).float().mean().item()\n",
    "                curve.append((trial_counter, acc_batch))\n",
    "\n",
    "            total += loss.item() * y.size(0)\n",
    "            cnt += y.size(0)\n",
    "\n",
    "        print(f\"[HardDetour] epoch={ep} loss={total/max(1,cnt):.3f}\")\n",
    "\n",
    "    # 評価\n",
    "    adapter.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(ds_ev)):\n",
    "            obs, y = ds_ev[i]\n",
    "            for k in obs:\n",
    "                obs[k] = obs[k].unsqueeze(0).to(device)\n",
    "            logits, _ = adapter(obs)\n",
    "            pred = logits.argmax(-1).item()\n",
    "            correct += int(pred == y)\n",
    "            total += 1\n",
    "\n",
    "    acc = correct/max(1,total)\n",
    "    print(f\"[HardDetour] eval accuracy = {acc:.3f}\")\n",
    "    return acc, curve\n",
    "\n",
    "\n",
    "def train_local_reflex(adapter: ModelAdapter, device=\"cpu\", epochs=4, batch_size=128, lr=1e-3):\n",
    "    ds_tr = LocalReflexDataset(n_samples=6000, device=device)\n",
    "    ds_ev = LocalReflexDataset(n_samples=1000, device=device)\n",
    "    dl = torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    opt = torch.optim.Adam(adapter.parameters(), lr=lr)\n",
    "    curve = []\n",
    "    trial_counter = 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        for obs, y in dl:\n",
    "            trial_counter += len(y)\n",
    "            for k in obs: obs[k] = obs[k].to(device)\n",
    "            y = y.to(device)\n",
    "            logits, _ = adapter(obs)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(-1)\n",
    "                acc_batch = (pred == y).float().mean().item()\n",
    "                curve.append((trial_counter, acc_batch))\n",
    "\n",
    "        print(f\"[LocalReflex] epoch={ep} loss={loss.item():.3f}\")\n",
    "\n",
    "    # 評価\n",
    "    adapter.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for obs, y in ds_ev:\n",
    "            for k in obs: obs[k] = obs[k].unsqueeze(0).to(device)\n",
    "            y = int(y.item())\n",
    "            logits, _ = adapter(obs)\n",
    "            pred = logits.argmax(-1).item()\n",
    "            correct += int(pred == y); total += 1\n",
    "    acc = correct/max(1,total)\n",
    "    print(f\"[LocalReflex] eval accuracy = {acc:.3f}\")\n",
    "    return acc, curve\n",
    "\n",
    "# ================== RPM-Mini (2x2) ==================\n",
    "class RPMMiniDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    2x2のRaven最小版（行:ルール→合成）。6x6グリッドを128次元に射影してvisionへ。\n",
    "    ルール: XNOR / XOR / COUNT のいずれか。選択肢は3つ。\n",
    "    \"\"\"\n",
    "    def __init__(self, n=6000, grid=6, device=\"cpu\", noise=0.05, mixer=None, seed=0):\n",
    "        super().__init__()\n",
    "        self.device=device; self.noise=noise; self.grid=grid\n",
    "        torch.manual_seed(seed); random.seed(seed)\n",
    "        self.mixer = mixer or FrozenObsMixer()\n",
    "        self.samples=[]\n",
    "        for _ in range(n):\n",
    "            rule = random.choice([\"XNOR\",\"XOR\",\"COUNT\"])\n",
    "            # タイルをバイナリ画像(6x6)で3つ作る（T00, T01, T10）。T11が欠損。\n",
    "            T00 = (torch.rand(grid,grid,device=device)>0.65).float()\n",
    "            # 次元ごとのトグル/シフト\n",
    "            mask = (torch.rand_like(T00)>0.5).float()\n",
    "            T01 = (T00 if rule==\"COUNT\" else torch.clamp(T00 + (1-mask),0,1))\n",
    "            T10 = torch.clamp(T00*mask,0,1) if rule!=\"COUNT\" else (torch.rand_like(T00)>0.65).float()\n",
    "\n",
    "            if rule==\"XNOR\":\n",
    "                T11 = 1.0 - torch.logical_xor(T01.bool(), T10.bool()).float()  # 一致＝1\n",
    "            elif rule==\"XOR\":\n",
    "                T11 = torch.logical_xor(T01.bool(), T10.bool()).float()\n",
    "            else:  # COUNT: 個数保存（T11の1の数＝T01の1の数）\n",
    "                ones_target = int(T01.sum().item())\n",
    "                flat = torch.zeros(grid*grid, device=device)\n",
    "                idx = torch.randperm(grid*grid, device=device)[:ones_target]\n",
    "                flat[idx]=1.0\n",
    "                T11 = flat.view(grid,grid)\n",
    "\n",
    "            def vec(img):\n",
    "                base = img.flatten().float()  # 36次元\n",
    "                # 36→V_BASE(=8)へ圧縮 → 128へ固定射影（FrozenObsMixerに合わせる）\n",
    "                # ここは単純に36→128へパディング&線形でも良いが、ノイズで多様性付与\n",
    "                v = torch.zeros(V_BASE, device=device)\n",
    "                take = min(V_BASE, base.numel())\n",
    "                v[:take] = base[:take]\n",
    "                V_full, _ = self.mixer(v.unsqueeze(0), torch.zeros(1,S_BASE,device=device))\n",
    "                return (V_full.squeeze(0) + self.noise*torch.randn(D_VISION,device=device))\n",
    "\n",
    "            # ビネット：T00,T01 / T10,  ?(T11)\n",
    "            panel = torch.stack([vec(T00), vec(T01), vec(T10)],0)  # (3,128)\n",
    "            vision = panel.mean(0)  # 簡単化：まとめて1ベクトルに符号化\n",
    "\n",
    "            # 候補の生成（正解1つ＋ダミー2つ）\n",
    "            correct_vec = vec(T11)\n",
    "            wrong1 = vec(T11.roll(shifts=1, dims=0))  # 適当な擾乱\n",
    "            wrong2 = vec(1.0 - T11)                   # 反転\n",
    "            # 候補を結合しsomatoにエンコード（3候補×キー付与）\n",
    "            choices = torch.stack([correct_vec, wrong1, wrong2],0)  # (3,128)\n",
    "            # ソマトに簡易インデックス（3）とノイズ\n",
    "            somato = torch.randn(D_SOMATO, device=device)*self.noise\n",
    "            # 正解位置をシャッフル\n",
    "            order = torch.randperm(3)\n",
    "            y = int((order==0).nonzero()[0])  # 元の0番(correct_vec)が今どこか\n",
    "            choices = choices[order]\n",
    "\n",
    "            # 観測辞書\n",
    "            obs = {\n",
    "                \"vision\": vision,                               # 128\n",
    "                \"somatosensory\": somato,                        # 60\n",
    "                \"olfaction\": torch.randn(D_OLFACT,device=device)*self.noise,\n",
    "                \"auditory\":  torch.randn(D_AUDIT, device=device)*self.noise,\n",
    "                \"proprioception\": torch.randn(D_PROP,device=device)*self.noise,\n",
    "            }\n",
    "            # 3候補はauditoryに混ぜると紛らわしいので、proprioの先頭で軽く埋め込む案もあるが、\n",
    "            # ここでは adapter が最終的に motor に集約するため choices をvisionに平均加算で提示\n",
    "            obs[\"vision\"] = obs[\"vision\"] + 0.25*choices.mean(0)\n",
    "\n",
    "            self.samples.append((obs, y))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i): return self.samples[i]\n",
    "\n",
    "\n",
    "def train_rpm_mini(adapter: ModelAdapter, device=\"cpu\", epochs=3, batch_size=128, noise=0.05):\n",
    "    ds_tr = RPMMiniDataset(6000, device=device, noise=noise)\n",
    "    ds_ev = RPMMiniDataset(1000, device=device, noise=noise, seed=7)\n",
    "    dl = torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    opt = torch.optim.Adam(adapter.parameters(), lr=1e-3)\n",
    "    curve=[]; trial=0\n",
    "    adapter.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        total=cnt=0\n",
    "        for obs,y in dl:\n",
    "            trial+=len(y)\n",
    "            for k in obs: obs[k]=obs[k].to(device)\n",
    "            y=y.to(device)\n",
    "            logits,_=adapter(obs)\n",
    "            loss=F.cross_entropy(logits,y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                acc=(logits.argmax(-1)==y).float().mean().item()\n",
    "                curve.append((trial,acc))\n",
    "            total+=loss.item()*y.size(0); cnt+=y.size(0)\n",
    "        print(f\"[RPM-Mini] epoch={ep} loss={total/max(1,cnt):.3f}\")\n",
    "    # eval\n",
    "    adapter.eval()\n",
    "    correct=total=0\n",
    "    with torch.no_grad():\n",
    "        for obs,y in ds_ev:\n",
    "            for k in obs: obs[k]=obs[k].unsqueeze(0).to(device)\n",
    "            y=int(y)\n",
    "            logits,_=adapter(obs)\n",
    "            pred=logits.argmax(-1).item()\n",
    "            correct+=int(pred==y); total+=1\n",
    "    acc=correct/max(1,total)\n",
    "    print(f\"[RPM-Mini] eval accuracy = {acc:.3f}\")\n",
    "    return acc, curve\n",
    "\n",
    "# ================== ARC-Mini ==================\n",
    "class ARCMiniDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ARCの代表規則を簡略化：majority-color / component-count / row-parity-flip\n",
    "    2つの入出力例を暗黙にvisionへ埋め込み、3つ目の正解出力を3択で問う。\n",
    "    \"\"\"\n",
    "    def __init__(self, n=6000, grid=6, k_colors=3, device=\"cpu\", noise=0.05, mixer=None, seed=1):\n",
    "        super().__init__()\n",
    "        self.device=device; self.noise=noise; self.grid=grid; self.k=k_colors\n",
    "        random.seed(seed); torch.manual_seed(seed)\n",
    "        self.mixer = mixer or FrozenObsMixer()\n",
    "        self.samples=[]\n",
    "        for _ in range(n):\n",
    "            rule = random.choice([\"MAJ\",\"CCNT\",\"PARITY\"])\n",
    "            def rand_grid():\n",
    "                return torch.randint(0,self.k,(grid,grid),device=device)\n",
    "\n",
    "            def apply(g):\n",
    "                if rule==\"MAJ\":\n",
    "                    # 全体で最多色に塗りつぶし\n",
    "                    vals,counts = torch.unique(g, return_counts=True)\n",
    "                    c = vals[torch.argmax(counts)].item()\n",
    "                    return torch.full_like(g, int(c))\n",
    "                elif rule==\"CCNT\":\n",
    "                    # 1の連結成分数を保つ(粗い：1色以外は0)\n",
    "                    bin = (g==1).float()\n",
    "                    # ここでは近似として総数をそのまま別の場所に散布\n",
    "                    ones=int(bin.sum().item())\n",
    "                    out=torch.zeros_like(g)\n",
    "                    idx=torch.randperm(g.numel(), device=device)[:ones]\n",
    "                    out.view(-1)[idx]=1\n",
    "                    return out.long()\n",
    "                else:  # PARITY\n",
    "                    # 偶数行:そのまま、奇数行:色を反転(mod k)\n",
    "                    out=g.clone()\n",
    "                    out[1::2]=(self.k-1 - out[1::2])%self.k\n",
    "                    return out\n",
    "\n",
    "            # 2つの例 (in_i -> out_i)\n",
    "            in1,in2 = rand_grid(), rand_grid()\n",
    "            out1,out2 = apply(in1), apply(in2)\n",
    "            # クエリ入力\n",
    "            in3 = rand_grid(); out3 = apply(in3)\n",
    "\n",
    "            def enc(g):\n",
    "                # 色をone-hot→平均→V_BASEへ落としてFrozenObsMixerで128へ\n",
    "                oh = F.one_hot(g, num_classes=self.k).float().mean(dim=(0,1))  # (k,)\n",
    "                v = torch.zeros(V_BASE, device=device); take=min(V_BASE, oh.numel())\n",
    "                v[:take]=oh[:take]\n",
    "                V_full,_=self.mixer(v.unsqueeze(0), torch.zeros(1,S_BASE,device=device))\n",
    "                return V_full.squeeze(0)+self.noise*torch.randn(D_VISION,device=device)\n",
    "\n",
    "            # 例をvisionに埋める（in,outを平均加算）\n",
    "            vision = enc(in1)+enc(out1)+enc(in2)+enc(out2)+0.5*enc(in3)\n",
    "\n",
    "            # 候補作成（正解＋2ダミー）\n",
    "            correct=enc(out3)\n",
    "            wrong1 = enc((out3+1)%self.k)  # 色シフト\n",
    "            wrong2 = enc(out3.roll(shifts=1,dims=0))  # 粗擾乱（意味なしでもOK）\n",
    "            choices=torch.stack([correct,wrong1,wrong2],0)\n",
    "            order=torch.randperm(3); y=int((order==0).nonzero()[0])\n",
    "            choices=choices[order]\n",
    "            vision = vision + 0.25*choices.mean(0)\n",
    "\n",
    "            obs={\n",
    "                \"vision\": vision,\n",
    "                \"somatosensory\": torch.randn(D_SOMATO,device=device)*self.noise,\n",
    "                \"olfaction\": torch.randn(D_OLFACT,device=device)*self.noise,\n",
    "                \"auditory\":  torch.randn(D_AUDIT, device=device)*self.noise,\n",
    "                \"proprioception\": torch.randn(D_PROP, device=device)*self.noise,\n",
    "            }\n",
    "            self.samples.append((obs, y))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self,i): return self.samples[i]\n",
    "\n",
    "\n",
    "def train_arc_mini(adapter: ModelAdapter, device=\"cpu\", epochs=3, batch_size=128, noise=0.05):\n",
    "    ds_tr=ARCMiniDataset(6000, device=device, noise=noise)\n",
    "    ds_ev=ARCMiniDataset(1000, device=device, noise=noise, seed=13)\n",
    "    dl=torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    opt=torch.optim.Adam(adapter.parameters(), lr=1e-3)\n",
    "    curve=[]; trials=0\n",
    "    adapter.train()\n",
    "    for ep in range(1,epochs+1):\n",
    "        total=cnt=0\n",
    "        for obs,y in dl:\n",
    "            trials+=len(y)\n",
    "            for k in obs: obs[k]=obs[k].to(device)\n",
    "            y=y.to(device)\n",
    "            logits,_=adapter(obs)\n",
    "            loss=F.cross_entropy(logits,y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                acc=(logits.argmax(-1)==y).float().mean().item()\n",
    "                curve.append((trials,acc))\n",
    "            total+=loss.item()*y.size(0); cnt+=y.size(0)\n",
    "        print(f\"[ARC-Mini] epoch={ep} loss={total/max(1,cnt):.3f}\")\n",
    "    # eval\n",
    "    adapter.eval()\n",
    "    correct=total=0\n",
    "    with torch.no_grad():\n",
    "        for obs,y in ds_ev:\n",
    "            for k in obs: obs[k]=obs[k].unsqueeze(0).to(device)\n",
    "            y=int(y)\n",
    "            logits,_=adapter(obs)\n",
    "            pred=logits.argmax(-1).item()\n",
    "            correct+=int(pred==y); total+=1\n",
    "    acc=correct/max(1,total)\n",
    "    print(f\"[ARC-Mini] eval accuracy = {acc:.3f}\")\n",
    "    return acc, curve\n",
    "\n",
    "# ================== Grid-Path-FirstStep ==================\n",
    "class GridPathFirstStep(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    7x7グリッド: S(スタート), G(ゴール), #(障害物)\n",
    "    ルール: 最短路の最初の一手 (L/F/R) を3択で答える。\n",
    "    \"\"\"\n",
    "    def __init__(self, n=8000, size=7, device=\"cpu\", noise=0.05, mixer=None, seed=2):\n",
    "        super().__init__()\n",
    "        self.device=device; self.noise=noise; self.N=size\n",
    "        random.seed(seed); torch.manual_seed(seed)\n",
    "        self.mixer = mixer or FrozenObsMixer()\n",
    "        self.samples=[]\n",
    "        for _ in range(n):\n",
    "            grid=torch.zeros(size,size,dtype=torch.long,device=device)\n",
    "            # S,G配置\n",
    "            sx,sy = random.randint(0,size-1), random.randint(0,size-1)\n",
    "            gx,gy = random.randint(0,size-1), random.randint(0,size-1)\n",
    "            while (gx,gy)==(sx,sy):\n",
    "                gx,gy = random.randint(0,size-1), random.randint(0,size-1)\n",
    "            grid[sy,sx]=1; grid[gy,gx]=2\n",
    "            # 障害物\n",
    "            for _o in range(random.randint(size//2, size)):\n",
    "                ox,oy = random.randint(0,size-1), random.randint(0,size-1)\n",
    "                if (ox,oy) not in [(sx,sy),(gx,gy)]:\n",
    "                    grid[oy,ox]=3  # '#'\n",
    "\n",
    "            # 最短路をBFSで探索し、一手目を決める\n",
    "            from collections import deque as _dq\n",
    "            dirs=[(1,0), (0,1), (-1,0), (0,-1)]  # E,S,W,N（右手系）\n",
    "            prev={}; q=_dq([(sx,sy)]); visited={(sx,sy)}\n",
    "            found=False\n",
    "            while q:\n",
    "                x,y=q.popleft()\n",
    "                if (x,y)==(gx,gy): found=True; break\n",
    "                for dx,dy in dirs:\n",
    "                    nx,ny=x+dx,y+dy\n",
    "                    if 0<=nx<size and 0<=ny<size and (nx,ny) not in visited and grid[ny,nx]!=3:\n",
    "                        visited.add((nx,ny)); prev[(nx,ny)]=(x,y); q.append((nx,ny))\n",
    "            if not found:\n",
    "                # 経路なし→ランダム方角（学習上はノイズだがロバスト性テストになる）\n",
    "                step_label=random.randint(0,2)\n",
    "            else:\n",
    "                # ゴールから戻って1手目を抽出\n",
    "                path=[]; cur=(gx,gy)\n",
    "                while cur!=(sx,sy):\n",
    "                    path.append(cur); cur=prev[cur]\n",
    "                path=path[::-1]\n",
    "                first=path[0] if path else (gx,gy)\n",
    "                dx,dy = first[0]-sx, first[1]-sy\n",
    "                # 現在の朝向: 東(1,0)とする。dx,dy→L/F/R\n",
    "                mapping={(1,0):1,(0,1):2,(-1,0):3,(0,-1):0}  # to dir idx\n",
    "                forward=(1,0)  # 基準\n",
    "                # dir to label: 左=0, 前=1, 右=2\n",
    "                if   (dx,dy)==forward: step_label=1\n",
    "                elif (dx,dy)==(0,-1): step_label=0  # 上は左に相当（基準から見て）\n",
    "                elif (dx,dy)==(0,1):  step_label=2  # 下は右\n",
    "                else: step_label=0 if random.random()<0.5 else 2\n",
    "\n",
    "            # 視覚エンコード（one-hot平均→FrozenObsMixer→128）\n",
    "            # 0:空,1:S,2:G,3:# のone-hot平均で荒い地図表現\n",
    "            oh=F.one_hot(grid, num_classes=4).float().mean(dim=(0,1))  # (4,)\n",
    "            v=torch.zeros(V_BASE,device=device); take=min(V_BASE,oh.numel())\n",
    "            v[:take]=oh[:take]\n",
    "            V_full,_=self.mixer(v.unsqueeze(0), torch.zeros(1,S_BASE,device=device))\n",
    "            vision=V_full.squeeze(0)+self.noise*torch.randn(D_VISION,device=device)\n",
    "\n",
    "            obs={\n",
    "                \"vision\": vision,\n",
    "                \"somatosensory\": torch.randn(D_SOMATO,device=device)*self.noise,\n",
    "                \"olfaction\": torch.randn(D_OLFACT,device=device)*self.noise,\n",
    "                \"auditory\":  torch.randn(D_AUDIT, device=device)*self.noise,\n",
    "                \"proprioception\": torch.randn(D_PROP, device=device)*self.noise,\n",
    "            }\n",
    "            self.samples.append((obs, step_label))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self,i): return self.samples[i]\n",
    "\n",
    "\n",
    "def train_grid_firststep(adapter: ModelAdapter, device=\"cpu\", epochs=3, batch_size=128, noise=0.05):\n",
    "    ds_tr=GridPathFirstStep(8000, device=device, noise=noise)\n",
    "    ds_ev=GridPathFirstStep(1200, device=device, noise=noise, seed=11)\n",
    "    dl=torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    opt=torch.optim.Adam(adapter.parameters(), lr=1e-3)\n",
    "    curve=[]; trials=0\n",
    "    adapter.train()\n",
    "    for ep in range(1,epochs+1):\n",
    "        total=cnt=0\n",
    "        for obs,y in dl:\n",
    "            trials+=len(y)\n",
    "            for k in obs: obs[k]=obs[k].to(device)\n",
    "            y=y.to(device)\n",
    "            logits,_=adapter(obs)\n",
    "            loss=F.cross_entropy(logits,y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                acc=(logits.argmax(-1)==y).float().mean().item()\n",
    "                curve.append((trials,acc))\n",
    "            total+=loss.item()*y.size(0); cnt+=y.size(0)\n",
    "        print(f\"[Grid-FirstStep] epoch={ep} loss={total/max(1,cnt):.3f}\")\n",
    "    # eval\n",
    "    adapter.eval()\n",
    "    correct=total=0\n",
    "    with torch.no_grad():\n",
    "        for obs,y in ds_ev:\n",
    "            for k in obs: obs[k]=obs[k].unsqueeze(0).to(device)\n",
    "            y=int(y)\n",
    "            logits,_=adapter(obs)\n",
    "            pred=logits.argmax(-1).item()\n",
    "            correct+=int(pred==y); total+=1\n",
    "    acc=correct/max(1,total)\n",
    "    print(f\"[Grid-FirstStep] eval accuracy = {acc:.3f}\")\n",
    "    return acc, curve\n",
    "\n",
    "\n",
    "def compute_tal_metrics(curve, criterion=0.85, window=100, budget=2000):\n",
    "    \"\"\"\n",
    "    curve: list of (trial_idx, acc)\n",
    "    return: TTC, AUC@B, Asy@B\n",
    "    \"\"\"\n",
    "    accs = [a for _,a in curve]\n",
    "    trials = [t for t,_ in curve]\n",
    "\n",
    "    # TTC\n",
    "    ttc = None\n",
    "    for i in range(window, len(accs)):\n",
    "        if sum(accs[i-window:i])/window >= criterion:\n",
    "            ttc = trials[i]\n",
    "            break\n",
    "\n",
    "    # AUC@B\n",
    "    auc = sum(accs[:budget]) / min(budget,len(accs))\n",
    "\n",
    "    # Asymptote\n",
    "    asy = sum(accs[-window:]) / min(window,len(accs))\n",
    "\n",
    "    return ttc, auc, asy\n",
    "\n",
    "\n",
    "def compute_efficiency(auc, params, trials):\n",
    "    \"\"\"Eff@B = AUC@B / (params * trials)\"\"\"\n",
    "    return auc / max(1,(params*trials))\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Build & Run\n",
    "# ===============================================================\n",
    "def build_models(device=\"cpu\") -> Dict[str, nn.Module]:\n",
    "    return {\n",
    "        \"1_Cnidarian\": CnidarianNerveNet(motor_dim=8).to(device),\n",
    "        \"2_SegmentedRestricted\": SegmentedGangliaRestricted(segments=6, motor_per_seg=2).to(device),\n",
    "        \"3_Cephalopod\": CephalopodBrainV3(motor_dim=4).to(device),\n",
    "        \"4_Fish\":      FishBrainV3(motor_dim=12).to(device),\n",
    "        \"5_Human\":     HumanCortexV4(motor_dim=20).to(device)\n",
    "    }\n",
    "\n",
    "def run_benchmark(device=None, jelly_epochs=3, rev_steps=1500, batch_size=128, detour_epochs=15):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    set_seed(0)\n",
    "    results = {}\n",
    "    models = build_models(device=device)\n",
    "    for name, base in models.items():\n",
    "        params = sum(p.numel() for p in base.parameters())\n",
    "        init_state = {k: v.detach().clone() for k, v in base.state_dict().items()}\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Model: {name}\")\n",
    "        # Jellyfish\n",
    "        base.load_state_dict(init_state, strict=True)\n",
    "        agent = ModelAdapter(base, n_actions=3, use_memory=False).to(device)\n",
    "        jelly_acc, jelly_curve = train_hd_jellyfish(agent, device=device, epochs=jelly_epochs, batch_size=batch_size)\n",
    "        j_ttc, j_auc, j_asy = compute_tal_metrics(jelly_curve, criterion=0.90, window=100, budget=2000)\n",
    "        j_eff = compute_efficiency(j_auc, params, trials=2000)\n",
    "\n",
    "        # Reversal\n",
    "        base.load_state_dict(init_state, strict=True)\n",
    "        agent = ModelAdapter(base, n_actions=2, use_memory=False).to(device)\n",
    "        pre, post, rev_curve = train_reversal(agent, device=device, steps=rev_steps)\n",
    "        r_ttc, r_auc, r_asy = compute_tal_metrics(rev_curve, criterion=0.80, window=200, budget=rev_steps)\n",
    "        r_eff = compute_efficiency(r_auc, params, trials=rev_steps)\n",
    "\n",
    "        # Detour\n",
    "        base.load_state_dict(init_state, strict=True)\n",
    "        agent = ModelAdapter(base, n_actions=3, use_memory=False).to(device)\n",
    "        detour_acc, detour_curve = train_detour(agent, device=device, epochs=detour_epochs, batch_size=batch_size)\n",
    "        d_ttc, d_auc, d_asy = compute_tal_metrics(detour_curve, criterion=0.85, window=200, budget=3000)\n",
    "        d_eff = compute_efficiency(d_auc, params, trials=3000)\n",
    "\n",
    "        # reflex\n",
    "        base.load_state_dict(init_state, strict=True)\n",
    "        agent = ModelAdapter(base, n_actions=3, use_memory=False).to(device)\n",
    "        acc, r_curve = train_local_reflex(agent, device=device, epochs=15)\n",
    "        r_ttc, r_auc, r_asy = compute_tal_metrics(r_curve, criterion=0.85, window=200, budget=3000)\n",
    "        d_eff = compute_efficiency(d_auc, params, trials=3000)\n",
    "\n",
    "        results[name] = {\n",
    "            \"jelly_acc\": jelly_acc, \"jelly_TTC\": j_ttc, \"jelly_AUC\": j_auc, \"jelly_Asy\": j_asy, \"jelly_Eff\": j_eff,\n",
    "            \"rev_pre\": pre, \"rev_post\": post, \"rev_TTC\": r_ttc, \"rev_AUC\": r_auc, \"rev_Asy\": r_asy, \"rev_Eff\": r_eff,\n",
    "            \"detour_acc\": detour_acc, \"detour_TTC\": d_ttc, \"detour_AUC\": d_auc, \"detour_Asy\": d_asy, \"detour_Eff\": d_eff,\n",
    "            \"reflex_acc\": acc, \"reflex_TTC\": r_ttc, \"reflex_AUC\": r_auc, \"reflex_Asy\": r_asy, \"reflex_Eff\": r_eff,\n",
    "        }\n",
    "\n",
    "        # RPM-Mini\n",
    "        base.load_state_dict(init_state, strict=True)\n",
    "        agent = ModelAdapter(base, n_actions=3, use_memory=False).to(device)\n",
    "        rpm_acc, rpm_curve = train_rpm_mini(agent, device=device, epochs=80, batch_size=batch_size)\n",
    "        rpm_ttc, rpm_auc, rpm_asy = compute_tal_metrics(rpm_curve, criterion=0.75, window=200, budget=3000)\n",
    "\n",
    "        # ARC-Mini\n",
    "        base.load_state_dict(init_state, strict=True)\n",
    "        agent = ModelAdapter(base, n_actions=3, use_memory=False).to(device)\n",
    "        arc_acc, arc_curve = train_arc_mini(agent, device=device, epochs=80, batch_size=batch_size)\n",
    "        arc_ttc, arc_auc, arc_asy = compute_tal_metrics(arc_curve, criterion=0.75, window=200, budget=3000)\n",
    "\n",
    "        # Grid-FirstStep\n",
    "        base.load_state_dict(init_state, strict=True)\n",
    "        agent = ModelAdapter(base, n_actions=3, use_memory=False).to(device)\n",
    "        gpf_acc, gpf_curve = train_grid_firststep(agent, device=device, epochs=50, batch_size=batch_size)\n",
    "        gpf_ttc, gpf_auc, gpf_asy = compute_tal_metrics(gpf_curve, criterion=0.75, window=200, budget=3000)\n",
    "\n",
    "        # 結果格納:\n",
    "        results[name].update({\n",
    "          \"rpm_acc\": rpm_acc, \"rpm_TTC\": rpm_ttc, \"rpm_AUC\": rpm_auc, \"rpm_Asy\": rpm_asy,\n",
    "          \"arc_acc\": arc_acc, \"arc_TTC\": arc_ttc, \"arc_AUC\": arc_auc, \"arc_Asy\": arc_asy,\n",
    "          \"gpf_acc\": gpf_acc, \"gpf_TTC\": gpf_ttc, \"gpf_AUC\": gpf_auc, \"gpf_Asy\": gpf_asy,\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"# Summary: jelly_acc | rev_pre | rev_post| detour_acc\")\n",
    "    print(\"#\"*60)\n",
    "    for k in results:\n",
    "        r = results[k]\n",
    "        print(k)\n",
    "        print(r)\n",
    "    return results\n",
    "\n",
    "# ===================== Run =====================\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    results = run_benchmark(device=device, jelly_epochs=5, rev_steps=1500, batch_size=128)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
